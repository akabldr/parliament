# 怎样实现一个高可用的分布式key-value存储服务
[代码](https://github.com/z42y/parliament)
## 起因
很多人将部署到多台电脑的系统一概称为分布式系统，并且为了让系统看起来更“分布式”，对系统做各种数据库分库、SOA、微服务化等。

分库分表、拆分部署等方案本质是为了性能和容量扩展，SOA则是系统对企业内外组织架构划分的妥协，至于现在很多所谓的微服务架构，我觉得还是硬件太便宜了。

对性能和容量的需求，促使系统必须使用多台机器，如何保证系统在多台机器上和在一台机器上的行为一样，才是分布式系统的问题范畴。

除了性能，另一个需要使用多台机器的原因是为了高可用，当某些机器或机房不可用时，其他机器可以继续提供服务，同样的，这需要保证主备之间的行为有某种程度的一致性预期。

如果服务器只是进行无状态的计算，比如对请求参数排序，这样是不会有一致性问题的，因为同一个排序算法，在所有机器上结果都应该一样。
不同的状态只能来自各种写操作，比如硬盘数据的更新、程序堆内存的更新等，这就涉及到分布式（读写）一致性的问题。

很多对分布式一致性、高可用的介绍偏向算法简介，无法体现其实现过程的各种细节，所以我在本人精力、能力范围内，尽量以生产环境要求，用java实现了一个key-value服务，配合这篇文档，结合代码注释讲解一个高可用分布式系统的设计和实现过程。

### kv服务功能
这个kv服务实现了PUT、GET、DEL、RANGE（范围查询）命令，使用redis的二进制协议为客户端提供服务，所以可以使用redis-cli连接服务器进行演示。
key排序使用字节数组的字典序。如：
```bash
bash#: redis-cli -p 18001
127.0.0.1:18001> put a A
(integer) 1
127.0.0.1:18001> put b B
(integer) 1
127.0.0.1:18001> range a c
1) "A"
2) "B"
127.0.0.1:18001> del b
(integer) 1
127.0.0.1:18001> get b
(nil)
```

### 运行要求及IDE配置
请查看代码库README文件。

## 引出问题

实现一个key-value单机服务只需三步☺：
1. 打开端口，监听客户端请求。
2. 接收请求，调用存储引擎处理。
3. 从网络返回处理结果。

随着客户增多，导致单机服务的容量、性能出现问题，好在客户之间数据没有任何关系，所以可以对不同客户在不同机房和机器上部署独立的kv服务实例，
只需在迁移时暂停一段时间服务
TODO 图

接着出现某个客户A，他的数据量很大，导致独立的服务器也无法满足容量、性能要求了，所以我们对存储进行切片（sharding)，把这个客户的数据通过某个规则（如key值范围）划分存储到不同服务器上，新增一个代理进程，负责切片路由的计算和转发。
TODO 图
这个架构的面临的挑战在于，如果新增切片，如何保证正确性和可用性。TODO 一致性hash

接着出现了某个客户B，数据量不大，但是要求99.9999%的服务可用等级。这意味着每年服务不可用的时间只能有32秒（32秒并不短，你不会希望在这32秒时，你正在抛售股票或者进行期货交割）。
一年时间里，服务器随时可能发生各种硬件故障，机房可能断电，网络可能发生故障，同时操作系统和软件本身也可能发生各种问题导致不可用。

这一切都意味着我们需要灾备（failover容灾）。灾备最基本的思想就是备份，备胎，备用电池，备用发动机等等。
软件系统的灾备难点在于备份需要和原系统保持同步。

以本key-value服务为例子，切换到备份后，可以成功get一个在原系统put成功的值，同时，如果原系统del了某个key，在备份系统对该key执行get应该返回nil。

### 主备实现

#### 增量同步
对于数据系统，最简单的同步方式就是在主备间拷贝数据。以本key-value系统为例，拷贝整个数据目录的时间延迟显然太大，即便精心设计增量拷贝机制，
在故障发生的时刻，也无法保证备份能够拿到最新的增量更改。

#### 


## 分布式一致性
### 各种一致性定义 

### CAP和PACELC

## 实现共识算法

## 实现复制状态机

## 实现网络协议

## 实现kv服务

## 实现可靠的存储引擎
