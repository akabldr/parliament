# 自制分布式缓存服务：设计及实现

源码及构建请参看[github项目主页](https://github.com/z42y/parliament/)和[javadoc参考](./javadoc/index.html)

# 功能及目标

该服务除了实现常见的"put _key_ _value_"、"get _key_"、"del _key0_  \[_key1_ ...\]"操作，
还实现了按范围取值的命令"range _begin_ _end_"，其中begin和end为开始、结束key值，字典序排序。

使用redis的resp协议进行通信，可以直接使用redis-cli连接测试，或者redis客户端库进行操作。

为了容错，允许用户使用多台机器同时提供服务，避免单点故障。当客户端发现某个服务地址不可用时，可以连接到另一个地址进行服务请求。

使用多台机器需要解决[一致性模型](https://zh.wikipedia.org/wiki/%E5%86%85%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B)问题。

这个服务保证**线性一致性**，对客户端来说，无论该系统有多少台机器，都和操作一台机器的效果一样。

反过来说，非线性一致的系统，对同一个key，在某段时间出现不同客户端读到不同的值，或者多次读的结果，其顺序与更新的顺序不一致，
比如某key先后被更新为a、b、c，某客户端在某时间三次读取的结果依次为b、c、a，而且另一个客户端三次读取的结果可能依次为c、b、a。

如果满足线性一致性，服务可以安全的用做leader选举、唯一值广播、分布式锁服务等。

例如：

- leader选举：put leader 'my ip'，无论多少个客户端并发写入leader，只会成功一个，其余客户端会返回put失败。
- 用户昵称绑定：put 'a nickname' 'user id', 某个昵称（'a nickname'）在全局只会被赋予唯一一个user id。

# 接收请求
因为redis的网络协议、客户端和库已经非常流行了，项目采用redis协议提供服务。

## JAVA NIO的使用
首先使用NIO接收客户连接，在连接成功的channel上挂载一个[RespReadHandler](./javadoc/io/github/parliament/resp/RespReadHandler.html)类，
使用[RespDecoder](./javadoc/io/github/parliament/resp/RespDecoder.html)类对异步到来的字节报文进行解码，RespReadHandler使用其get方法，判断是否解码完成。

解码完成后，使用[KeyValueEngine](./javadoc/io/github/parliament/kv/class-use/KeyValueEngine.html)进行真正的缓存读写处理，

处理完成后，新建一个[RespWriteHandler](./javadoc/io/github/parliament/resp/RespWriteHandler.html)将结果返回给客户端，
接着重新挂载一个RespReadHandler进行下一个请求处理。

重新生成RespWriterHandler和RespReadHandler是为了方便进行GC，当然可以手工管理各种buffer的回收和重利用，这里不做详细设计了。

因为缓存的对象都比较小，[KeyValueEngine](./javadoc/io/github/parliament/kv/class-use/KeyValueEngine.html)并没有使用InputStream之类的模式进一步提升异步性能。

## 网络协议的解析构造
[RespDecoder](./javadoc/io/github/parliament/resp/RespDecoder.html)是redis的[RESP协议](https://redis.io/topics/protocol)解码器，
RESP一共有四种数据类型：
- SIMPLE_STRING 字符串
- ERROR 错误字符串
- INTEGER 整数
- BULK_STRING 二进制字节组
- ARRAY 数组

除了ARRAY可以包含其他类型和其他ARRAY，处理稍稍麻烦外，其他类型都容易解析。

使用JAVA标准库中ByteBuffer类直接解析协议是比较困难的，因为报文的写入和读取是并发的，不可能等到报文读取完成后，才开始解析，
甚至无法知道报文什么时候结束。

另外，报文处理往往需要"回溯"操作，从之前某个位置重新开始解析。使用ByteBuffer的flip和rewind、reset太底层，抽象层次不够。

所以通过实现自己的[ByteBuf](./javadoc/io/github/parliament/resp/ByteBuf.html)进行报文解析，主要提供了独立的读写index，方便回溯和读写操作分离。
底层使用byte[]保存数据，也可以使用direct allocate的ByteBuffer提升性能，但是ByteBuf的生命周期短、数据量都小，无法体现其优势。

# 解决一致性问题
前面说到服务需要保证线性一致，如果[KeyValueEngine](./javadoc/io/github/parliament/kv/class-use/KeyValueEngine.html)直接对key-value进行读写，将会导致不一致的问题。

举个简单的例子：

- 开始时，客户端A和客户端B都连接了服务S1.
- 客户端A的网络出现问题，与S1的连接断开，切换到服务S2，客户端B保持不变。
- 客户端A和B对同一个key进行不同更新。
- 该key在S1和S2出现冲突，违背了一致性保证。

很多key-value服务（如亚马逊的DynamoDB）采用多写+多读（写节点+读节点>节点总数）的方式冲突发现，采用[逻辑时钟](https://en.wikipedia.org/wiki/Logical_clock)解决冲突，保证最终读一致，
对于昵称的例子，仅仅是最终读一致的保证可能导致某个昵称在某一段时间被两个用户同时使用。

保证线性一致性的常见思想是复制状态机。

## 复制状态机
复制状态机思想是将程序看做一个状态机，在某个状态下，对某个确定的输入，程序的下一个状态是确定的。

举个例子，对两个数据库进行完全相同的一系列读写操作后，这两个数据库的数据一定是一样的（当然不能使用如current_timestamp之类的sql函数）。

同样的，在缓存服务中，所有节点的初始状态是一样的，如果所有客户端的读写操作，在所有节点上按相同顺序进行执行，那么任何一个操作完成时，所有节点状态一定是一致的。

## 全序广播（原子广播）及共识
多个客户端会发生并发操作，服务器无法区分这些操作的先后顺序，因为硬件时钟并不可靠，即便存在一个可靠的时钟，网络的延迟也导致操作在不同节点有不同的到达顺序。

同时，节点在收到一个操作请求时，如何确定是否有之前的操作尚未到达？

所以需要对操作分配一个全局递增的编号，确定操作的顺序，同时所有节点对在这个编号的操作达成一致。

这是一个典型的分布式共识问题：对某一个提案的内容达成一致。

协商**每个**编号操作内容的过程，就是**一次**分布式共识达成过程，因此全序广播问题等价为共识问题。

## 共识算法及paxos算法推导
先尝试设计一个共识算法。

直观的，节点a对其他节点发起一个提案，如果接收者同意该提案，返回t，否则返回f。
如所有接收者返回t，则节点a通知所有接收者提案通过，所有节点执行该提案内容，否则节点a通知提案取消。显然接收者需要满足**约束P1**：

    节点必须同意最早收到的提案，否则不会有任何提案通过。

以上方案实际类似于[两阶段提交算法](https://zh.wikipedia.org/zh-hans/%E4%BA%8C%E9%98%B6%E6%AE%B5%E6%8F%90%E4%BA%A4)，
该方法最大的问题是存在单点故障，在提案请求阶段，任何一个节点失败，都会导致提案无法通过。
因为两阶段提交是为了保证各个参与者的任务要么都成功，要么都取消，而不是为了保证高可用。

为了高可用（这是使用多个节点的原因！），必须容忍部分节点的网络故障或进程失败。

但如果网络分裂为A和B两个网络，A和B不能互通，一个提案在A通过，另一个提案在B网络通过，则会出现不一致的情况。
解决方案是只通过获得超过一半（大多数）节点同意的提案。

但是A和B网络可能有重叠，此时某些节点可能会收到两个或两个以上的提案，这些节点必须能够接受这些两个及以上的提案，因为按照约束P1，
这些提案可能是其他节点的第一个提案，已经被接受了。

为了区分提案，需要为提案分配编号，比如发起节点的本地序列号+ip地址。这里不要混淆提案的编号和复制状态机输入的编号，
状态机每个编号（依次递增）的输入内容对应一次共识过程，该共识过程可能存在多个编号（只需保证单调）的提案。

这就引出**约束P2**：

    在一次共识过程中，一旦某个提案值v被通过，所有通过的更高编号的提案值也是v。

提案被通过，表示至少被一个节点接受过。所以加强P2，得到约束P2a：

    一旦某个提案值v获得批准，被任何节点接受的更高编号的提案值也是v。
    
因为通信是异步的，一个从休眠或故障恢复的节点，给某个尚未收到任何提案的节点，提交一个更高编号的不同提案v1，按照约束P1，该节点必须接收该提案，
这就违背了约束P2a，所以，与其约束接收者，约束提交者更加方便，对P2a加强约束，得到约束P2b：

    一旦某个提案值v获得批准，任何发起者发起的更高编号的提案值也是v。

我们通过证明P2b是如何确保只有一个提案值被通过的，来推导保证P2b的方法。

假设编号为m（m < n）到n-1的提案值都为v，如果m提案被选中，那么存在一个大多数节点的集合C接收了m提案，这意味着：

    C中每个节点接收了一个m到n-1的某个提案，m到n-1的每个提案值都是v。

任何包含大部分节点的集合S，和C至少有一个共同节点。可以通过以下约束P2c保证n提案值为v：

    编号为n的提案v被提交，则存在一个节点集合S，S中没有任何节点接收过小于n的提案。 或者，v是S中所有节点接收过的提案中，编号比n小的那些提案中，编号最大的提案的值。


# 持久化
## SkipList算法
## Page管理
