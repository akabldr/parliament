# 怎样实现一个高可用的分布式key-value存储服务
[代码](https://github.com/z42y/parliament)
## 起因
很多人将部署到多台电脑的系统一概称为分布式系统，并且为了让系统看起来更“分布式”，对系统做各种数据库分库、SOA、微服务化等。

分库分表、拆分部署等方案本质是为了性能和容量扩展，SOA则是系统对企业内外组织架构划分的妥协，至于现在很多所谓的微服务架构，我觉得还是硬件太便宜了。

对性能和容量的需求，促使系统必须使用多台机器，如何保证系统在多台机器上和在一台机器上的行为一样，才是分布式系统的问题范畴。

除了性能，另一个需要使用多台机器的原因是为了高可用，当某些机器或机房不可用时，其他机器可以继续提供服务，同样的，这需要保证主备之间的行为有某种程度的一致性预期。

如果服务器只是进行无状态的计算，比如对请求参数排序，这样是不会有一致性问题的，因为同一个排序算法，在所有机器上结果都应该一样。
不同的状态只能来自各种写操作，比如硬盘数据的更新、程序堆内存的更新等，这就涉及到分布式（读写）一致性的问题。

很多对分布式一致性、高可用的介绍偏向算法简介，无法体现其实现过程的各种细节，所以我在本人精力、能力范围内，尽量以生产环境要求，用java实现了一个key-value服务，配合这篇文档，结合代码注释讲解一个高可用分布式系统的设计和实现过程。

### kv服务功能
这个kv服务实现了PUT、GET、DEL、RANGE（范围查询）命令，使用redis的二进制协议为客户端提供服务，所以可以使用redis-cli连接服务器进行演示。
key排序使用字节数组的字典序。如：
```bash
bash#: redis-cli -p 18001
127.0.0.1:18001> put a A
(integer) 1
127.0.0.1:18001> put b B
(integer) 1
127.0.0.1:18001> range a c
1) "A"
2) "B"
127.0.0.1:18001> del b
(integer) 1
127.0.0.1:18001> get b
(nil)
```

### 运行要求及IDE配置
请查看代码库README文件。

## 引出问题

实现一个key-value单机服务只需三步☺：
1. 打开端口，监听客户端请求。
2. 接收请求，调用存储引擎处理。
3. 从网络返回处理结果。

随着客户增多，导致单机服务的容量、性能出现问题，好在客户之间数据没有任何关系，所以可以对不同客户在不同机房和机器上部署独立的kv服务实例，
只需在迁移时暂停一段时间服务
TODO 图

某些客户的数据量越来越大，导致独立的服务器也无法满足容量、性能要求了，所以我们对存储进行切片（sharding)，把这个客户的数据通过某个规则（如key值范围）划分存储到不同服务器上，新增一个代理进程，负责切片路由的计算和转发。
TODO 图
这个架构的面临的挑战在于，如果新增切片，如何保证正确性和可用性。TODO 一致性hash

某些客户数据量不大，但是要求服务有99.9999%的可用性，这意味着每年服务不可用的时间只能有32秒（32秒并不短，想象一下金融交易系统或者航空系统）。

其余时间，服务不光能要处理请求，还要保证**正确**和**可容忍的时延**。

以上任何一种架构，在服务周期内遇到任何硬件故障、电源故障、网络故障、软件故障都会停止服务。面对这种客户要求，该怎么做？

虽然故障发生的概率很高，但是两个设备或系统同时发生的概率时很低的。

这一切都意味着我们需要灾备（failover），或故障切换能力。灾备最基本的思想就是备份，备胎，备用电池，备用发动机等等。
软件系统的灾备难点在于备份需要和原系统保持某种同步或共识。

以本key-value服务为例子，切换到备份后，可以成功get一个在原系统put成功的值，同时，如果原系统del了某个key，在备份系统对该key执行get应该返回nil。

接着客户B的数据逐渐增大，需要采用和客户A一样的切片技术进行负载均衡，同时对SAL的要求保持不变。

### 主备实现方式
#### 异步同步
对于数据系统，最简单的同步方式就是在主备间拷贝数据。以本key-value系统为例，拷贝整个数据目录的时间延迟显然太大，即便精心设计增量拷贝机制（如redo log），
由于网络的固有的延迟，在故障发生的时刻，也无法保证备份能够拿到最新的增量更改。

#### 副本 TODO 更好的名字
在类似磁盘阵列（RAID）的技术中，一次写操作会往多个副本同时写入，至少保证一个成功。我们假设失败副本的替换或修复过程无需停机。

但是网络系统和RAID这种单机硬件有所区别：
1. 一个副本无法确认另一个副本到底时宕机还是他们之间有网络故障。
2. 可能出现某些客户可以访问某个副本集合A，而另一些客户只能访问副本集合B，A和B可能并不相交。即所谓网裂问题。

为了提高可用率，我们必然允许任何副本在任何时间都可以提供服务。

那么可能出现几种冲突：
1. 


那么当两个副本的数据出现冲突时，该怎么办？




## 分布式一致性
### 各种一致性定义 

### CAP和PACELC

## 实现共识算法

## 实现复制状态机

## 实现网络协议

## 实现kv服务

## 实现可靠的存储引擎
